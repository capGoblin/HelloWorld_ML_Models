{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bkt4Fbq0U51D",
        "outputId": "8ad5f207-f1d0-40b3-cdc9-cb5cee3dae0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tf-nightly\n",
            "  Downloading tf_nightly-2.14.0.dev20230629-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (490.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m490.8/490.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (23.5.26)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.56.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (3.8.0)\n",
            "Collecting keras-nightly~=2.14.0.dev (from tf-nightly)\n",
            "  Downloading keras_nightly-2.14.0.dev2023062907-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (16.0.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (0.2.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.16.0)\n",
            "Collecting tb-nightly~=2.14.0.a (from tf-nightly)\n",
            "  Downloading tb_nightly-2.14.0a20230629-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m71.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (2.3.0)\n",
            "Collecting tf-estimator-nightly~=2.14.0.dev (from tf-nightly)\n",
            "  Downloading tf_estimator_nightly-2.14.0.dev2023062908-py2.py3-none-any.whl (440 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m441.0/441.0 kB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions<4.6.0,>=3.6.6 (from tf-nightly)\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tf-nightly) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tf-nightly) (0.40.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.14.0.a->tf-nightly) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.14.0.a->tf-nightly) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.14.0.a->tf-nightly) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.14.0.a->tf-nightly) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.14.0.a->tf-nightly) (0.7.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tb-nightly~=2.14.0.a->tf-nightly) (2.3.6)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.14.0.a->tf-nightly) (5.3.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.14.0.a->tf-nightly) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tb-nightly~=2.14.0.a->tf-nightly) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tb-nightly~=2.14.0.a->tf-nightly) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.14.0.a->tf-nightly) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.14.0.a->tf-nightly) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.14.0.a->tf-nightly) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tb-nightly~=2.14.0.a->tf-nightly) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tb-nightly~=2.14.0.a->tf-nightly) (2.1.3)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tb-nightly~=2.14.0.a->tf-nightly) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tb-nightly~=2.14.0.a->tf-nightly) (3.2.2)\n",
            "Installing collected packages: typing-extensions, tf-estimator-nightly, keras-nightly, tb-nightly, tf-nightly\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.6.3\n",
            "    Uninstalling typing_extensions-4.6.3:\n",
            "      Successfully uninstalled typing_extensions-4.6.3\n",
            "Successfully installed keras-nightly-2.14.0.dev2023062907 tb-nightly-2.14.0a20230629 tf-estimator-nightly-2.14.0.dev2023062908 tf-nightly-2.14.0.dev20230629 typing-extensions-4.5.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/distributions/distribution.py:259: ReparameterizationType.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.10/dist-packages/tensorflow/python/ops/distributions/bernoulli.py:165: RegisterKL.__init__ (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-datasets in /usr/local/lib/python3.10/dist-packages (4.9.2)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.4.0)\n",
            "Requirement already satisfied: array-record in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.4.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (8.1.3)\n",
            "Requirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.1.8)\n",
            "Requirement already satisfied: etils[enp,epath]>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.22.4)\n",
            "Requirement already satisfied: promise in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.3)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (3.20.3)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (5.9.5)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.27.1)\n",
            "Requirement already satisfied: tensorflow-metadata in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.13.1)\n",
            "Requirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (2.3.0)\n",
            "Requirement already satisfied: toml in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (0.10.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (4.65.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from tensorflow-datasets) (1.14.1)\n",
            "Requirement already satisfied: importlib_resources in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (5.12.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (4.5.0)\n",
            "Requirement already satisfied: zipp in /usr/local/lib/python3.10/dist-packages (from etils[enp,epath]>=0.9.0->tensorflow-datasets) (3.15.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->tensorflow-datasets) (3.4)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from promise->tensorflow-datasets) (1.16.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2,>=1.52.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow-metadata->tensorflow-datasets) (1.59.1)\n",
            "2.14.0-dev20230629\n"
          ]
        }
      ],
      "source": [
        "# import libraries\n",
        "try:\n",
        "  # %tensorflow_version only exists in Colab.\n",
        "  !pip install tf-nightly\n",
        "except Exception:\n",
        "  pass\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "!pip install tensorflow-datasets\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "import warnings\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from matplotlib.colors import ListedColormap\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import precision_score, recall_score, classification_report, accuracy_score, f1_score\n",
        "from sklearn.metrics import ConfusionMatrixDisplay\n",
        "# plot_confusion_matrix\n",
        "from nltk.tokenize import word_tokenize\n",
        "import scipy.sparse as sp"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTb33sC-U8Wf",
        "outputId": "b796be15-3c1c-481f-f14a-d98a22aca9f4"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get data files\n",
        "!wget https://cdn.freecodecamp.org/project-data/sms/train-data.tsv\n",
        "!wget https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv\n",
        "\n",
        "train_file_path = \"train-data.tsv\"\n",
        "test_file_path = \"valid-data.tsv\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ez1NIhqoVDle",
        "outputId": "0cf53666-4d5c-4cbe-fd93-e7a46763653c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-06-29 13:27:47--  https://cdn.freecodecamp.org/project-data/sms/train-data.tsv\n",
            "Resolving cdn.freecodecamp.org (cdn.freecodecamp.org)... 172.67.70.149, 104.26.3.33, 104.26.2.33, ...\n",
            "Connecting to cdn.freecodecamp.org (cdn.freecodecamp.org)|172.67.70.149|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 358233 (350K) [text/tab-separated-values]\n",
            "Saving to: ‘train-data.tsv’\n",
            "\n",
            "train-data.tsv      100%[===================>] 349.84K  --.-KB/s    in 0.08s   \n",
            "\n",
            "2023-06-29 13:27:47 (4.04 MB/s) - ‘train-data.tsv’ saved [358233/358233]\n",
            "\n",
            "--2023-06-29 13:27:47--  https://cdn.freecodecamp.org/project-data/sms/valid-data.tsv\n",
            "Resolving cdn.freecodecamp.org (cdn.freecodecamp.org)... 172.67.70.149, 104.26.3.33, 104.26.2.33, ...\n",
            "Connecting to cdn.freecodecamp.org (cdn.freecodecamp.org)|172.67.70.149|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 118774 (116K) [text/tab-separated-values]\n",
            "Saving to: ‘valid-data.tsv’\n",
            "\n",
            "valid-data.tsv      100%[===================>] 115.99K  --.-KB/s    in 0.01s   \n",
            "\n",
            "2023-06-29 13:27:47 (8.19 MB/s) - ‘valid-data.tsv’ saved [118774/118774]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train = pd.read_csv('train-data.tsv', delimiter='\\t')\n",
        "df_test = pd.read_csv('valid-data.tsv', delimiter='\\t')\n",
        "df_train.columns = ['spam/ham', 'message']\n",
        "df_test.columns = ['spam/ham', 'message']"
      ],
      "metadata": {
        "id": "ngaimWBoVFRf"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_train)\n",
        "print(df_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Al3JTjp2VK5u",
        "outputId": "1174fd05-8dca-41b7-f0cb-3cda6e4d7357"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     spam/ham                                            message\n",
            "0         ham                           you can never do nothing\n",
            "1         ham  now u sound like manky scouse boy steve,like! ...\n",
            "2         ham  mum say we wan to go then go... then she can s...\n",
            "3         ham  never y lei... i v lazy... got wat? dat day ü ...\n",
            "4         ham  in xam hall boy asked girl tell me the startin...\n",
            "...       ...                                                ...\n",
            "4173      ham  just woke up. yeesh its late. but i didn't fal...\n",
            "4174      ham  what do u reckon as need 2 arrange transport i...\n",
            "4175     spam  free entry into our £250 weekly competition ju...\n",
            "4176     spam  -pls stop bootydelious (32/f) is inviting you ...\n",
            "4177      ham  tell my  bad character which u dnt lik in me. ...\n",
            "\n",
            "[4178 rows x 2 columns]\n",
            "     spam/ham                                            message\n",
            "0         ham         not much, just some textin'. how bout you?\n",
            "1         ham  i probably won't eat at all today. i think i'm...\n",
            "2         ham  don‘t give a flying monkeys wot they think and...\n",
            "3         ham                                who are you seeing?\n",
            "4         ham  your opinion about me? 1. over 2. jada 3. kusr...\n",
            "...       ...                                                ...\n",
            "1386      ham  true dear..i sat to pray evening and felt so.s...\n",
            "1387      ham               what will we do in the shower, baby?\n",
            "1388      ham  where are you ? what are you doing ? are yuou ...\n",
            "1389     spam  ur cash-balance is currently 500 pounds - to m...\n",
            "1390     spam  not heard from u4 a while. call 4 rude chat pr...\n",
            "\n",
            "[1391 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_train['message'] = df_train['message'].astype(str)\n",
        "df_test['message'] = df_test['message'].astype(str)"
      ],
      "metadata": {
        "id": "zUOKq2VZVK79"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adding a column of numbers of charachters,words and sentences in each msg\n",
        "\n",
        "df_train[\"No_of_Characters\"] = df_train[\"message\"].apply(len)\n",
        "df_train[\"No_of_Words\"]=df_train.apply(lambda row: nltk.word_tokenize(row[\"message\"]), axis=1).apply(len)\n",
        "df_train[\"No_of_sentence\"]=df_train.apply(lambda row: nltk.sent_tokenize(row[\"message\"]), axis=1).apply(len)\n",
        "\n",
        "df_train.describe().T"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "ROq6fqkbVK-a",
        "outputId": "6ed86260-bc82-448f-fd38-4a6acb848bc9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                   count       mean        std  min   25%   50%    75%    max\n",
              "No_of_Characters  4178.0  80.458114  60.285037  2.0  35.0  62.0  122.0  910.0\n",
              "No_of_Words       4178.0  18.780517  13.825062  1.0   9.0  15.0   27.0  216.0\n",
              "No_of_sentence    4178.0   1.895883   1.421113  1.0   1.0   1.0    2.0   34.0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c97fbf4a-05e5-4dac-be4e-3ac179e6b9f3\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>No_of_Characters</th>\n",
              "      <td>4178.0</td>\n",
              "      <td>80.458114</td>\n",
              "      <td>60.285037</td>\n",
              "      <td>2.0</td>\n",
              "      <td>35.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>910.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>No_of_Words</th>\n",
              "      <td>4178.0</td>\n",
              "      <td>18.780517</td>\n",
              "      <td>13.825062</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>27.0</td>\n",
              "      <td>216.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>No_of_sentence</th>\n",
              "      <td>4178.0</td>\n",
              "      <td>1.895883</td>\n",
              "      <td>1.421113</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>34.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c97fbf4a-05e5-4dac-be4e-3ac179e6b9f3')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c97fbf4a-05e5-4dac-be4e-3ac179e6b9f3 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c97fbf4a-05e5-4dac-be4e-3ac179e6b9f3');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_msg(text):\n",
        "  remove_non_alpha = re.sub('[^a-zA-Z]', ' ', text)\n",
        "  remove_non_alpha = remove_non_alpha.lower()\n",
        "  return remove_non_alpha\n",
        "\n",
        "\n",
        "df_train['message'] = df_train['message'].apply(clean_msg)\n",
        "df_test['message'] = df_test['message'].apply(clean_msg)"
      ],
      "metadata": {
        "id": "dTtuXYIKVLAj"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(df_train)\n",
        "print(df_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DHoTF0ahVLCt",
        "outputId": "6aa9cfe9-d27d-45b8-e1d6-d312479ccda4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     spam/ham                                            message  \\\n",
            "0         ham                           you can never do nothing   \n",
            "1         ham  now u sound like manky scouse boy steve like  ...   \n",
            "2         ham  mum say we wan to go then go    then she can s...   \n",
            "3         ham  never y lei    i v lazy    got wat  dat day   ...   \n",
            "4         ham  in xam hall boy asked girl tell me the startin...   \n",
            "...       ...                                                ...   \n",
            "4173      ham  just woke up  yeesh its late  but i didn t fal...   \n",
            "4174      ham  what do u reckon as need   arrange transport i...   \n",
            "4175     spam  free entry into our      weekly competition ju...   \n",
            "4176     spam   pls stop bootydelious     f  is inviting you ...   \n",
            "4177      ham  tell my  bad character which u dnt lik in me  ...   \n",
            "\n",
            "      No_of_Characters  No_of_Words  No_of_sentence  \n",
            "0                   24            5               1  \n",
            "1                  115           26               2  \n",
            "2                   83           19               1  \n",
            "3                   77           22               2  \n",
            "4                  185           38               2  \n",
            "...                ...          ...             ...  \n",
            "4173                74           25               3  \n",
            "4174                69           17               1  \n",
            "4175               106           21               2  \n",
            "4176               152           29               3  \n",
            "4177               165           44               4  \n",
            "\n",
            "[4178 rows x 5 columns]\n",
            "     spam/ham                                            message\n",
            "0         ham         not much  just some textin   how bout you \n",
            "1         ham  i probably won t eat at all today  i think i m...\n",
            "2         ham  don t give a flying monkeys wot they think and...\n",
            "3         ham                                who are you seeing \n",
            "4         ham  your opinion about me     over    jada    kusr...\n",
            "...       ...                                                ...\n",
            "1386      ham  true dear  i sat to pray evening and felt so s...\n",
            "1387      ham               what will we do in the shower  baby \n",
            "1388      ham  where are you   what are you doing   are yuou ...\n",
            "1389     spam  ur cash balance is currently     pounds   to m...\n",
            "1390     spam  not heard from u  a while  call   rude chat pr...\n",
            "\n",
            "[1391 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(df_train['message'][0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jE1D5k29VLE8",
        "outputId": "70d7b7c9-efab-47bb-85f8-b56a3fa68d06"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "str"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T6r-sv9-VLGy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XE7n_ih_VLLP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "paQbalWnVLNP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NS7UljF-VLPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zyM0cW-SVLRz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fiYuSKIhVLUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wW7XiZpuVLWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_933CqL6VLYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lTAWN0OPVLbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D3z_hMs0VLdb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qfm0r1pXVLhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BMOCjxAyVLkE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nQ7R6rrnVLmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "NIn3-XgkVLo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Js3FCp0nVLrU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nTAcqhSdVLty"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}